---
title: "HW3"
author: "Marco Navarro, Asha Christensen, Pranjal Maheshka"
date: "2023-03-26"
output: md_document
---
```{r setup1, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(modelr)
library(mosaic)
library(psych)
library(rsample)  
library(caret)
library(parallel)
library(foreach)
library(gamlr)
library(verification)
library(groupdata2) 
library(rpart)
library(randomForest)
library(pdp)
```

## Question 1: What Causes What?

1. Cities with high levels of crime are incentivized to hire more cops: this means that even if cops deter crime, regressing "crime" on "police" may give an opposite result, since crime and police are positively correlated. Therefore, we need to use a different method to find the true effect of police on crime.


2. They found times in which there was a high police presence on the street which was not due to street crime: the terror alert level being orange or red. When this is the case, there is an extra police presence that is due to terrorism but not due to the type of crime they wanted to study. In this way, they could look at the effect of increased police presence on these days on street crime rates. 

The table shows that high alert terrorism days decreased the number of daily crimes by 7.316 crimes, and this effect was still large and significant after controlling for metro mid-day ridership.


3. There are 2 potential effects of a high terrorism alert (i) there is an increased police presence on the street, (ii) the terrorism threat reduces tourism and therefore potential victims on the street. Both of these could reduce the daily crime rate, so in order to isolate the first effect, we need to control for the third. Therefore, they use Metro mid-day ridership as an indicator of the number of people out-and-about. Controlling for this should isolate the effect of increased police presence on daily crime rates.


4. When we separate out crimes that occur in the national mall versus the rest of the city, we find that the reduction in crime due to high alert days primarily takes place in the national mall-- this may be due to the fact that the majority of increased police presence during a terrorism alert is concentrated on the national mall since these are the most likely terrorism threats. The decrease in crimes for the rest of the city is not statistically significant, whereas the reduction in crimes in the national mall is 2.62 crimes.


## Question 3 Predictive model building: green certification

```{r, echo=FALSE}
greendf = read.csv("greenbuildings.csv", as.is = T, fill = T, header = T)
#change this during upload? don't know how to run it tbh

greendf = greendf %>%
  mutate(revsqftyr = Rent*leasing_rate)

green_split = initial_split(greendf, prop = 0.8)
green_train = training(green_split)
green_test = testing(green_split)

```

Let's start with some step-wise models, using some medium linear models. Here are our initial out-of-sample RMSEs for both linear models:

``` {r, echo=FALSE}

lm1 = lm(revsqftyr ~ green_rating + size + age + renovated + amenities + City_Market_Rent, data=green_train)

lm2 = lm(revsqftyr ~ green_rating + cd_total_07 + hd_total07 + Gas_Costs + Electricity_Costs, data=green_train)

rmse(lm1, green_test)
rmse(lm2, green_test)

```
In these step-wise regressions, we allow the use of all variables except rent, leasing rate, property ID, LEED and Energystar, allowing for second-level interactions.

``` {r, echo=FALSE, include=FALSE}

step1 = step(lm1, scope=~(.-Rent - leasing_rate - CS_PropertyID -LEED - Energystar)^2)

step2 = step(lm2, scope=~(.-Rent - leasing_rate - CS_PropertyID -LEED - Energystar)^2)

```

```{r, echo=FALSE}
rmse(step1, green_test)
rmse(step2, green_test)

```
There's a slight improvement on out-of-sample RMSE, but not a large decrease. We may be able to use a random forest model to more accurately predict revenue per square foot per year.


```{r, echo=FALSE, cache=TRUE}

green_train_noNA = na.omit(green_train)

revenue.forest = randomForest(revsqftyr ~ (.-Rent - leasing_rate - CS_PropertyID -LEED - Energystar), data=green_train_noNA, importance = TRUE)

```

```{r, echo=FALSE}

green_test_noNA = na.omit(green_test)

modelr::rmse(revenue.forest, green_test_noNA)

```

Our out-of-sample RMSE is much lower than the linear step models produced, so this is our best model to predict revenue per square foot per year. We can now look at the partial effect of green rating.

```{r, echo=FALSE}
varImpPlot(revenue.forest)

```

According to the variable importance plot, this indicates that green rating is not one of the more important variables in predicting revenue per square foot per year.

```{r, echo=FALSE}
partialPlot(revenue.forest, green_test_noNA, green_rating)
```

We use our testing set to look at the partial effect of green rating. Green rating is a binary variable: we can see in the graph that being rated green is associated with an increase of roughly $80 in revenue per square foot per year. We can look to see if this partial effect is impacted by interaction with other relevant terms: Gas Costs, Electricity Costs and Size of the building.

```{r, echo=FALSE}
partial(revenue.forest, pred.var = c("green_rating", "size"), plot=TRUE)

partial(revenue.forest, pred.var = c("green_rating", "Electricity_Costs"), plot=TRUE)

```

We can see that there is not a large partial effect of green rating among buildings that are the same size: this may indicate that our original $80 of increased revenue may be due to a confounder. Perhaps large buildings are more likely to be rated green and also more likely to have increased revenue per square foot per year. Additionally, there is little to no effect of being rated green if energy costs such as electricity are sufficiently low.

## Question 4 Predictive model building: California housing

```{r setup4, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(gamlr)
library(tidyverse)
library(ggmap)
library(ggplot2)
library(tidycensus)
library(tigris)
library(sf)
library(rgdal)
library(broom)
library(maptools)
library(modelr)
library(mosaic)
library(psych)
library(rsample)  
library(caret)
library(parallel)
library(foreach)
library(verification)
library(groupdata2)
library(ggrepel)
library(rpart)
library(rpart.plot)
library(randomForest)
library(scales)
```

The median house value at the census tract level prediction exercise has been carried out using data on 20,640 census tracts in the state of California. In addition to median house value, this dataset contains 8 variables that capture different characteristics of the census tracks and their houses, such as population, number of households, median age in years of all residential households, total of bedrooms, etc. Before testing different methods, we standardize some variables, such as total bedrooms and total rooms.

### Median house value prediction strategy

Four different methods were tested in the building process of a prediction model of house prices: linear model, K-nearest neighbors regression, regression tree and random forests.

For the K-nearest neighbors regression method, different values of K were evaluated and we picked the one with the the minimum root mean square error (RMSE) after k-fold cross validation. On the other hand, recursive partitioning was applied to generate a sequence of trees and select the regression tree with the minimum cross-validated error.

### Model evaluation of the different methods and results

The k-fold cross validation is the procedure used to evaluate the out-of-sample performance of the linear model, the K-nearest neighbors regression and the regression tree. This is a resampling method that uses different portions of the data to test and train a model on different iterations. The whole dataset was split into 10 different folds to perform this evaluation and the accuracy of the prediction was evaluated by checking the root mean square error (RMSE).Among these three different methods, the tree regression model presents the best out-of-sample performance. The cross validated error for this tree model equals 59,621.

In the case of the random forest, due to the estimation process, we already have a measure of the out-of-sample performance with the “out-of-bag” predictions. However, we split our observations in a training and test set several times and calculate the root mean squared error for every partition in order to have a comparable out of sample error. Using this approach, the average RMSE on the test sets equals 51,375.

In conclusion, the results have shown that the random forest model has the best performance among these methods. Given this particular set of explanatory variables, the prediction model should be the random forest. 
The accuracy of these methods can be reevaluated after an expansion of the number of observations or number of explanatory variables.

Brief description of the best method: Random forests involve the creation of several decision trees. These decision trees are models composed of a collection of "questions" organized hierarchically that lead to a prediction. In the random forest model, each tree is trained with a different resampling of the available data and a randomly selected subset of the available features. After this first stage, the algorithm combines the predictions of all the created decision trees to provide a final prediction.

After selecting the best model, we proceed to present  graphs of the original values, random forest predicted values and random forest model's error.

```{r , include=FALSE}
################################### Read dataset
CAhousing = read.csv("CAhousing.csv")

########### Scale some variables
CAhousing2 = CAhousing  %>%
  mutate(meanrooms = totalRooms/households,meanbedrooms=totalBedrooms/households)
CAhousing2 = CAhousing2[,-c(4,5)]

################################## Linear model and KNN

########## Create folds for cross-validation  
K_folds = 10
CAhousing2 = CAhousing2 %>%
  mutate(fold_id = rep(1:K_folds, length=nrow(CAhousing2)) %>% sample)

###################### Linear model

##### Error matrix
err_save2 = matrix(0, nrow=1, ncol=10)

##### Linear regression

for(i in 1:K_folds) {
  train_set = which(CAhousing2$fold_id != i)
  y_test = CAhousing2$medianHouseValue[-train_set]
  lm1 = lm(medianHouseValue ~ (.-fold_id)^2 + I(housingMedianAge^2)+ I(medianIncome^2) , data=CAhousing2[train_set,])
   # Predictions out of sample
    # Root mean squared error
  err_save2[1, i] =rmse(lm1, data=CAhousing2[-train_set,])
}

##### Error matrix output

err_save2 = as.data.frame(err_save2)
err_save2 = err_save2 %>%
           mutate(err_save2, mean = (V1+V2+V3+V4+V5+V6+V7+V8+V9+V10)/K_folds)

############################################### KNN

########### Prepare dataset

CAhousing2knn = CAhousing2

#### Scale numerical variables

CAhousing2knn[, c("longitude", "latitude", "housingMedianAge", "population", "households", "medianIncome", "meanrooms", "meanbedrooms")] <- scale(CAhousing2knn[, c("longitude", "latitude", "housingMedianAge", "population", "households", "medianIncome", "meanrooms", "meanbedrooms")])

#### Create an auxiliary matrix

CAhousing2knn_folds = crossv_kfold(CAhousing2knn, k=K_folds)

##### We can run this across a range of k

k_grid = c(2, 4, 6, 8, 10, 12, 15,17, 20,22, 25,27, 30, 35, 40, 45,
           50, 60, 70, 80, 90, 100)

############ KNN regression

cv_grid = foreach(k = k_grid, .combine='rbind') %dopar% {
  models = map(CAhousing2knn_folds$train, ~ knnreg(medianHouseValue ~ .-fold_id , k=k, data = ., use.all=FALSE))
  errs = map2_dbl(models, CAhousing2knn_folds$test, modelr::rmse)
  c(k=k, errs, err = mean(errs), std_err = sd(errs)/sqrt(K_folds))
} %>% as.data.frame

############################################### Trees 

########## Tree regression

cahousing.tree2 = rpart(medianHouseValue ~ .-fold_id , data=CAhousing2,
                   control = rpart.control(cp = 0.00001))

#### cross-validated error 

cx1 = as.data.frame(printcp(cahousing.tree2))

######### Separate Cross-fold validation for tree with minimum standard error

Err_save3 = matrix(0, nrow=1, ncol=10)

for(i in 1:K_folds) {
  train_set = which(CAhousing2$fold_id != i)
  y_test = CAhousing2$medianHouseValue[-train_set]
  cahousing.tree_min = rpart(medianHouseValue ~ .-fold_id , 
  data=CAhousing2[train_set,],control = rpart.control(cp = 0.00006003052))
# Predictions out of sample
# Root mean squared error
  Err_save3[1, i] =rmse(cahousing.tree_min, data=CAhousing2[-train_set,])
}

##### Error matrix output

Err_save3 = as.data.frame(Err_save3)
Err_save3 = Err_save3 %>%
  mutate(Err_save3, mean = (V1+V2+V3+V4+V5+V6+V7+V8+V9+V10)/K_folds)

######################################### Random forest

###########         Split data into training and testing

CAhousing2_split =  initial_split(CAhousing2, prop=0.8)
CAhousing2_train = training(CAhousing2_split)
CAhousing2_test  = testing(CAhousing2_split)

############### Random forest code

CAhousing2.forest = randomForest(medianHouseValue ~ .-fold_id,
                                 data=CAhousing2_train, importance = TRUE)

############################### Compare RMSE  FOREST

modelr::rmse(CAhousing2.forest, CAhousing2_test)

##### Split train/test set several times for random forest (very time consuming)

#rmse_10 = do(10)*{
  # Split data into training and testing sets
#  load_split =  initial_split(CAhousing2, prop=0.8)
#  load_train = training(load_split)
#  load_test  = testing(load_split)
  
  # Random forest
#  load.forest = randomForest(medianHouseValue ~ .-fold_id,
#                             data=load_train, na.action=na.exclude)
  
  # Calculate RMSE
#  modelr::rmse(load.forest, load_test)}
#colMeans(rmse_10)


###############################   Predictions and errors

CAhousing3 = CAhousing2 %>%
  mutate(value_predt = predict(cahousing.tree_min,CAhousing2),pred_errort=(medianHouseValue-value_predt)^2)
 
CAhousing3 = CAhousing3 %>%
  mutate(value_predrf = predict(CAhousing2.forest,CAhousing2),pred_errorrfsq=(medianHouseValue-value_predrf)^2)

sqrt(mean(CAhousing3$pred_errort))

sqrt(mean(CAhousing3$pred_errorrfsq))
```

```{r , include=FALSE, warning=FALSE,message=FALSE}
#### Counties map

CA_tracts <- counties("CA")
```

## Original values

```{r , echo=FALSE, warning=FALSE,message=FALSE}
############################### Graphs

#### Additional variable for errors

CAhousing3 = CAhousing3 %>%
  mutate(pred_errorrf=medianHouseValue-value_predrf)

#### Original values

ggplot(CA_tracts) +
  geom_sf() + 
  geom_point(data=CAhousing3,aes(x=longitude,y=latitude,color=medianHouseValue),size= 1.2) +
  scale_colour_viridis_c(option = "E",labels = comma)+
  theme_bw() +
  theme(legend.key.size = unit(0.5, 'cm'),legend.position = c(0.8, 0.8))+
  labs(y = "", x = "", title = "Median House Value (Dollars)")+
  theme(legend.title = element_blank(),legend.text = element_text(size = 8, colour = "black",face = "bold"))+
  theme(axis.text.y = element_blank(),axis.text.x = element_blank())+
  theme(plot.title = element_text(hjust = 0.5))
```

## Predicted Values

```{r , echo=FALSE, warning=FALSE}
#### Predicted Values

ggplot(CA_tracts) +
  geom_sf() + 
  geom_point(data=CAhousing3,aes(x=longitude,y=latitude,color=value_predrf),size= 1.2) +
  scale_colour_viridis_c(option = "E",labels = comma)+
  theme_bw() +
  theme(legend.key.size = unit(0.5, 'cm'),legend.position = c(0.8, 0.8))+
  labs(y = "", x = "", title = "Median House Value (Dollars)")+
  theme(legend.title = element_blank(),legend.text = element_text(size = 8, colour = "black",face = "bold"))+
  theme(axis.text.y = element_blank(),axis.text.x = element_blank())+
  theme(plot.title = element_text(hjust = 0.5))
```

## Model's errors (Original values - Predicted values)

```{r , echo=FALSE, warning=FALSE}
#### Errors graph

ggplot(CA_tracts) +
  geom_sf() + 
  geom_point(data=CAhousing3,aes(x=longitude,y=latitude,color=pred_errorrf),size= 1.2) +
 scale_colour_viridis_c(limit=c(-50000,50000),label=comma,option = "H")+
  theme_bw() +
  theme(legend.key.size = unit(0.5, 'cm'),legend.position = c(0.8, 0.8))+
  labs(y = "", x = "", title = "Errors (Dollars)")+
  theme(legend.title = element_blank(),legend.text = element_text(size = 8, colour = "black",face="bold"))+
  theme(axis.text.y = element_blank(),axis.text.x = element_blank())+
  theme(plot.title = element_text(hjust = 0.5))

```

In the error graph, grey points correspond to those errors that are higher than 50,000 or lower than -50,000.



---
title: "v1--A Silent Epidemic: Overprescription of Opioids"
author: "Pranjal Maheshka, Asha Christensen, Marco Navarro"
date: "2023-04-24"
output: pdf_document
---
```{r test1}
startt=Sys.time()
# rmarkdown::render("project_v3.Rmd")
load(file = "data1.RData")
```

## Abstract
The opioid epidemic in the United States is a problem that has only recently seen sharper corrective action including the 2016 CDC guidelines on opioid prescription. Using CDC data, we developed a random forests model and a logistic regression model to predict opioid prescription during 2018-19. These models were then used to predict prescriptions from 2013-2016 and the difference between actual data and the model’s predictions were used to determine the overprescription of opioids. The most important variables were determined to be age, reported painscale, and pre-existing chronic conditions. The model had **low specificity** and was unsuccessful at predictions, likely attributed to the unexplained variation in patient characteristics not captured by the data set or due to inherent randomness in opioid prescriptions that were conducted without a formal framework. **REVIEW**

## Introduction
The overprescription of opioids has become a growing concern in the United States, with an estimated [1.6 million](https://www.hhs.gov/opioids/statistics/index.html#:~:text=Facts%20about%20Drug%20Overdose,epidemic%20data%20from%20the%20CDC) Americans struggling with opioid addiction. There were over [70,000 opioid overdose related deaths in 2021](https://nida.nih.gov/research-topics/trends-statistics/overdose-death-rates). In 2016, the Centers for Disease Control and Prevention (CDC) released [guidelines](https://www.cdc.gov/mmwr/volumes/65/rr/rr6501e1.htm) for prescribing opioids for chronic pain, aiming to address the issue of overprescription. However, the effectiveness of these guidelines in reducing opioid prescriptions and opioid-related harm is yet to be fully understood. In this project, we will use machine learning models in R to analyze the impact of the 2016 CDC guidelines (treated as the correct standard for prescription in our approach) on opioid prescription rates and the associated harm. We will explore various factors that contribute to overprescription of opioids, such as patient socio-demographics, medical conditions, diagnoses, and treatment characteristics. The goal of this project is to train a model on data after the CDC published new guidelines and then use this model to find the level of over-prescription of opioids in the previous years.  

#### Overview of 2016 CDC Guidelines
The CDC guidelines for prescribing opioids for chronic pain, published in March 2016, provide recommendations to healthcare providers to improve patient safety and reduce the risk of opioid-related harms. The guidelines recommend the use of non-opioid therapies as the preferred treatment for chronic pain and suggest that opioids should only be prescribed after considering other treatment options. The guidelines also recommend that healthcare providers establish treatment goals and regularly monitor patients for benefits and harms of opioid therapy. Additionally, the guidelines suggest limiting the duration of opioid therapy to three days or less for acute pain and to the lowest effective dose for chronic pain. Finally, the guidelines provide recommendations for mitigating the risks associated with opioid therapy, such as using prescription drug monitoring programs and assessing patients for risk factors for opioid misuse or addiction.


```{r setup, include=FALSE, eval = FALSE}
library(ggplot2)
library(dplyr)
library(rsample)
library(caret)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(rsample) 
library(randomForest)
library(lubridate)
library(modelr)
library(verification)
library(gamlr)
knitr::opts_chunk$set(echo = TRUE)

# setwd = ("C:/Users/pranj/Documents/Final-Project-Data/") #Pranjal
# setwd = ("C:/Users/ashac/OneDrive/Documents/GitHub/eco395m-sp23-DataMining/Final-Project/")
opioid_df = read.csv("data/data_final.csv") 
```

## Methodology 
This section provides an overview of the methodology used for analysis including steps on data collection, processing, and running the predictive models to replicate our results. STATA was used for primary data cleaning and R was used for data processing and implementing machine learning models. 

### Data Collection and Processing
[Source]( https://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/NHAMCS/): National Hospital Ambulatory Medical Care Survey (NHAMCS)
* The National Hospital Ambulatory Medical Care Survey (NHAMCS) collects data on the utilization and provision of ambulatory care services in hospital emergency and outpatient departments and ambulatory surgery locations. This is a national sample of visits to the emergency departments,  outpatient departments, and ambulatory surgery locations of noninstitutional general and short-stay hospitals.
* Data was downloaded for 2013-2019. The default file format is .dta (STATA dataset). Each year contains about 20,000 observations across 1000 variables including information on perscribed medication. The codebook found on the CDC website also labelled medications with their generic names, 9 of which are the generic names of opioids (fentanyl, buprenorphine, hydromorphone, hydrocodone, methadone, morphine, tapentadol, oxycodone, and codeine) according to the American Society of Addition Medicine.
* Pre-processing was done in STATA using a [cleaning script](https://github.com/pranjalmaheshka/eco395m-sp23-DataMining/tree/main/Final-Project/stata) provided by the CDC and then the files were exported as .csv’s in order to process them in R. 
* In order to reproduce the results, save the csv files were saved locally due to their size and then run [data_cleaning.r](https://github.com/pranjalmaheshka/eco395m-sp23-DataMining/blob/main/Final-Project/data_cleaning.R). This file subsets the data down to relevant socio-demographic features, medical diagnoses, patient’s medical history, and type of care provided. 
+ Information about prescribed medication is cross-referenced with CDC data to determine which patients were given opioids including type and potency which were saved as `opioid` (=1 if prescribed an opioid), `potency` (measured relative to morphine = 1), `opioids` (total number of opioids prescribed). Two opioid classification information files needed to run the data cleaning script can be found [here]( https://github.com/pranjalmaheshka/eco395m-sp23-DataMining/tree/main/Final-Project/data). 
* The final cleaned file is available [here]( https://github.com/pranjalmaheshka/eco395m-sp23-DataMining/tree/main/Final-Project/data). 
The final data set includes 145,630 observations across 94 variables including 3 opioid variables generated. 
```{r data_wrangling, include=FALSE, cache=TRUE, eval = FALSE}
factor(opioid_df$VDAYR)
factor(opioid_df$VMONTH)
factor(opioid_df$RESIDNCE)
opioid_df$SEX<-ifelse(opioid_df$SEX=="Female",1,0)
factor(opioid_df$RACEUN)
factor(opioid_df$REGION)
factor(opioid_df$ETHIM)
factor(opioid_df$IMMEDR)
factor(opioid_df$PAYTYPER)

opioid_df$AGE<-ifelse(opioid_df$AGE=="Under one year",1, opioid_df$AGE)
opioid_df$AGE<-ifelse(opioid_df$AGE=="93 years and over",93, opioid_df$AGE)

opioid_df$PAINSCALE<-ifelse(opioid_df$PAINSCALE=="Blank"|opioid_df$PAINSCALE=="Unknown",0,opioid_df$PAINSCALE)
opioid_df$PAINSCALE<-as.numeric(opioid_df$PAINSCALE)
opioid_df$AGE = as.numeric(opioid_df$AGE)

opioid_df = opioid_df %>%
  mutate(pre2016 = ifelse(YEAR < 2016 | (YEAR==2016 & 
                                           (VMONTH == 'January' | VMONTH == 'February')), 1, 0),
         PAYMCARE = ifelse(PAYMCARE == "Yes", 1, 0),
         PAYPRIV = ifelse(PAYPRIV == "Yes", 1, 0))

opioid_df = opioid_df%>%
  mutate(DIAG1 = ifelse(is.na(DIAG1), "BLANK", DIAG1))
opioid_df = opioid_df%>%
  mutate(DIAG2 = ifelse(is.na(DIAG2), "BLANK", DIAG2))
opioid_df = opioid_df%>%
  mutate(DIAG3 = ifelse(is.na(DIAG3), "BLANK", DIAG3))

factor(opioid_df$DIAG1)
factor(opioid_df$DIAG2)
factor(opioid_df$DIAG3)
factor(opioid_df$RFV1)
factor(opioid_df$RFV2)
factor(opioid_df$RFV3)
```

```{r random_forests1, include=TRUE, warning=FALSE, echo=FALSE, cache=TRUE, eval = FALSE}
##### POST Regulation modelling #####
opioid_model = subset(opioid_df, YEAR == 2018 | YEAR == 2019)
opioid_model = na.omit(opioid_model)
opioid_split =  initial_split(opioid_model, prop=0.8)
traindata = training(opioid_split)
testdata  = testing(opioid_split)

#### Random forest
load.forest = randomForest(opioid ~ . -ETHUN-PATCODE-BDATEFL-SEXFL
                           -ETHNICFL-RACERFL-RACER-RACERETH-AGEDAYS-AGER-PAYPRIV
                           -PAYMCARE-PAYMCAID-PAYWKCMP-PAYSELF 
                           -PAYNOCHG-PAYOTH-PAYDK-opioid-opioids-potency-pre2016,
                           data=traindata, importance = TRUE, ntree=5)
```

### Model Selection
Random Forests: Chosen because it adds more randomness to our sample data the way we would expect to see in the real world. We consider a total of 83 features in the final model design so using random forests helps subset the number of features and test different combinations of these features effectively. 

**Model Training:** The data show that opioid prescriptions are relatively stable in 2018-2019 relative to the previous years. There might be a lag in broader adoption of the 2016 CDC guidelines and the prescriptions drop from 2013-2018. Therefore, 2018-2019 data were taken as “ideal” prescription rates and the model was trained on this data using an 80/20 train test split. The model was then run on data from 2013 - Feb 2016 *CDC guidelines were published in March 2016) to determine over-prescription of opioids in the past.   

**Model Evaluation:** It is worth mentioning that this is an severely imbalanced classification problem, since "Opioid Prescription = 0" is the most likely outcome, with less than 5% of patients getting prescribed an opioid in 2018-2019. Consequently, that percentage sets our baseline or null model, the one that guesses "not opioid prescription" for every observation in the test set.

After choosing the optimal threshold, we evaluate the out-of-sample performance of our classifier by looking at the confusion matrix,which tabulates predicted status versus true status, for the test set.

*ADD CONFUSION MATRIX HERE*
```{r random_forests_treshold, include=TRUE}
endt=Sys.time()
endt - startt

## Confusion matrix (we need to decide cutoff)

testdata$predict_opioid = predict(load.forest, testdata)

testdata = testdata %>%
  mutate(predict_opioid = ifelse(predict_opioid >= 0.17, 1, 0), 
         false_neg = ifelse(predict_opioid < opioid, 1, 0))

table(real_opioid=testdata$opioid, predict_opioid=testdata$predict_opioid)

## ROC Curve
x <- testdata$opioid
y <- testdata$predict_opioid
rocdata <- data.frame(x,y)
roc.plot(rocdata$x, rocdata$y, show.thres=FALSE)
```
We can calculate from the table above that our random forest model has an 83% out-of-sample accuracy rate. Therefore, it is worse compared to the null model. A detailed discussion can be found in the next section. 

Additionally, we present here some popular metrics obtained from the results of the confusion matrix. 

* Sensitivity or recall = 0.41
* Specificity or false positive rate = 0.86
* Precision = 0.14


*The over-prescribed and the accurately non-prescribed**

The purpose of this segment is to identify probable systematic overprescription to a specific types of people in the pre 2016 Guideline period. Therefore, we apply our prediction model, trained and tested with 2018 and 2019 data, to the initial period from 2013 to February 2016. This subset of our date contains 72,199 observations.

*Confusion matrix*

After that, we define the group of the overprescribed patients as patients who received a prescription but should not have received it according to the prediction of our model. In addition, we define a second group of patients, accurately non prescribed, that includes people who did not receive an opioid prescription and our model also predict they should not have received one.

Having identified these two groups of patients, we proceed to calculate summary statistics of some variables we consider relevant. Our initial assumption is that we should not observed differences in these summary statistics for these two groups and The results are presented in the table below.

## Results

```{r, echo=FALSE, warning=FALSE}
## Opioid prescription plot

opioid_df %>%
  group_by(YEAR) %>%
  summarize(perc_opioid = mean(opioid)*100) %>%
  ggplot() +
  geom_col(aes(x=YEAR, y=perc_opioid))

```

```{r, echo=FALSE}
# Variable importance measures
vi = varImpPlot(load.forest, type=1)
```

*VI PLOT* 
```{r, echo=FALSE}
# Partial dependence plots
partialPlot(load.forest, testdata, 'PAINSCALE', las=1)
partialPlot(load.forest, testdata, 'AGE', las=1)
```

*PD Plot Discuss*

```{r, cache=TRUE}
## PRE 2016
pre16_df= opioid_df %>%
   filter(pre2016== 1)

pre16_df$predict_opioid = predict(load.forest, pre16_df)

pre16_df = pre16_df %>%
  mutate(predict_opioid = ifelse(predict_opioid >= 0.20, 1, 0), 
         false_neg = ifelse(predict_opioid < opioid, 1, 0),
         correct_neg = ifelse(predict_opioid == 0 & opioid==0, 1, 0))

table(real_opioid=pre16_df$opioid, predict_opioid=pre16_df$predict_opioid)

pre16_df = pre16_df %>%
  mutate(homeless = ifelse(RESIDNCE == 'Homeless' | RESIDNCE == "Homeless/homeless shelter", 1, 0),
         hispanic = ifelse(ETHIM == "Hispanic or Latino", 1, 0),
         black = ifelse(RACEUN == "Black/African American Only", 1, 0),
         white = ifelse(RACEUN == "White Only", 1, 0),
         asian = ifelse(RACEUN == "Asian Only", 1, 0))

false = pre16_df %>%
  filter(false_neg == 1) %>%
  summarize(perc_female = mean(SEX)*100, 
            avg_age = mean(AGE),
            perc_mcare = mean(PAYMCARE)*100,
            perc_privins = mean(PAYPRIV)*100,
            avg_pain = mean(PAINSCALE),
            perc_homeless = mean(homeless)*100,
            perc_hisp = mean(hispanic)*100,
            perc_black = mean(black)*100,
            perc_white = mean(white)*100,
            perc_asian = mean(asian)*100)


correct = pre16_df %>%
  filter(correct_neg == 1) %>%
  summarize(perc_female = mean(SEX)*100, 
            avg_age = mean(AGE),
            perc_mcare = mean(PAYMCARE)*100,
            perc_privins = mean(PAYPRIV)*100,
            avg_pain = mean(PAINSCALE),
            perc_homeless = mean(homeless)*100,
            perc_hisp = mean(hispanic)*100,
            perc_black = mean(black)*100,
            perc_white = mean(white)*100,
            perc_asian = mean(asian)*100)

df = tribble(~Outcome, ~Falsely_Prescribed, ~Correct_Nonprescribed,
        'Avg. Age', false$avg_age, correct$avg_age,
        '% Female', false$perc_female, correct$perc_female,
        '% Medicare', false$perc_mcare, correct$perc_mcare,
        '% Private Insurance', false$perc_privins, correct$perc_privins,
        '% Homeless', false$perc_homeless, correct$perc_homeless,
        'Avg. Pain', false$avg_pain, correct$avg_pain,
        '% Black', false$perc_black, correct$perc_black,
        '% White', false$perc_white, correct$perc_white,
        '% Asian', false$perc_asian, correct$perc_asian,
        '% Hispanic', false$perc_hisp, correct$perc_hisp)

df$Difference = df$Falsely_Prescribed- df$Correct_Nonprescribed

```

#### Drawbacks
There are many limitations with the scope of these results arising from a variety of factors. Primarily, these limitations stem from the modeling itself because our out-of-bag errors (running the model on the test set for 2018-2019) data show that there is a sharp trade off between sensitivity and 

## Conclusion
The opioid crisis has seen some respite by the suits against both manufacturers like Johnson & Johnson ([$26 billion settlement in 2021]( https://www.naag.org/issues/opioids/)) and retailers like Walmart ([$3.1 billion settlement in 2022]( https://www.nytimes.com/2022/11/15/health/walmart-opioids-settlement.html)) and some relief with changing prescription guidelines, the development of non-opioid pain relievers, and general awareness amongst the general public and medical practitioners. 

•	Our model build process and drawbacks

•	Main takeaways

#### Future Considerations

## Appendix
Optional

```{r}
endt=Sys.time()
endt - startt
```


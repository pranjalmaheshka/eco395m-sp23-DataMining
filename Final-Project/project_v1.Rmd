---
title: "A Silent Epidemic: Overprescription of Opioids"
author: "Pranjal Maheshka, Asha Christensen, Marco Navarro"
date: "2023-04-24"
output: html_document
---

## Abstract
The opioid epidemic in the United States is a problem that has only recently seen sharper corrective action including the 2016 CDC guidelines on opioid prescription. Using CDC data, we developed a random forests model and a logistic regression model to predict opioid prescription during 2018-19. These models were then used to predict prescriptions from 2013-2016 and the difference between actual data and the model’s predictions were used to determine the overprescription of opioids. The most important variables were determined to be age, reported painscale, and pre-existing chronic conditions. The model had **low specificity** and was unsuccessful at predictions, likely attributed to the unexplained variation in patient characteristics not captured by the data set or due to inherent randomness in opioid prescriptions that were conducted without a formal framework. **REVIEW**

## Introduction
  The overprescription of opioids has become a growing concern in the United States, with an estimated [1.6 million](https://www.hhs.gov/opioids/statistics/index.html#:~:text=Facts%20about%20Drug%20Overdose,epidemic%20data%20from%20the%20CDC) Americans struggling with opioid addiction. There were over [70,000 opioid overdose related deaths in 2021](https://nida.nih.gov/research-topics/trends-statistics/overdose-death-rates). In 2016, the Centers for Disease Control and Prevention (CDC) released [guidelines](https://www.cdc.gov/mmwr/volumes/65/rr/rr6501e1.htm) for prescribing opioids for chronic pain, aiming to address the issue of overprescription. However, the effectiveness of these guidelines in reducing opioid prescriptions and opioid-related harm is yet to be fully understood. In this project, we will use machine learning models in R to analyze the impact of the 2016 CDC guidelines (treated as the correct standard for prescription in our approach) on opioid prescription rates and the associated harm. We will explore various factors that contribute to overprescription of opioids, such as patient socio-demographics, medical conditions, diagnoses, and treatment characteristics. The goal of this project is to train a model on data after the CDC published new guidelines and then use this model to find the level of over-prescription of opioids in the previous years.  

#### Overview of 2016 CDC Guidelines
The CDC guidelines for prescribing opioids for chronic pain, published in March 2016, provide recommendations to healthcare providers to improve patient safety and reduce the risk of opioid-related harms. The guidelines recommend the use of non-opioid therapies as the preferred treatment for chronic pain and suggest that opioids should only be prescribed after considering other treatment options. The guidelines also recommend that healthcare providers establish treatment goals and regularly monitor patients for benefits and harms of opioid therapy. Additionally, the guidelines suggest limiting the duration of opioid therapy to three days or less for acute pain and to the lowest effective dose for chronic pain. Finally, the guidelines provide recommendations for mitigating the risks associated with opioid therapy, such as using prescription drug monitoring programs and assessing patients for risk factors for opioid misuse or addiction.


```{r setup, include=FALSE}
library(ggplot2)
library(dplyr)
library(rsample)
library(caret)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(rsample) 
library(randomForest)
library(lubridate)
library(modelr)
library(verification)
knitr::opts_chunk$set(echo = TRUE)

setwd = ("C:/Users/pranj/Documents/Final-Project-Data/") #Pranjal
setwd = ("C:/Users/ashac/OneDrive/Documents/GitHub/eco395m-sp23-DataMining/Final-Project/")
opioid_df = read.csv("data/data_final.csv") 
```

## Methodology 
This section provides an overview of the methodology used for analysis including steps on data collection, processing, and running the predictive models to replicate our results. STATA was used for primary data cleaning and R was used for data processing and implementing machine learning models. 

### Data Collection and Processing
[Source]( https://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/NHAMCS/): National Hospital Ambulatory Medical Care Survey (NHAMCS)
* Data was downloaded for 2013-2019. The default file format is .dta (STATA dataset). Each year contains about 20,000 observations across 1000 variables including information on the prescription of opioids. 
* Pre-processing was done in STATA using a [cleaning script](https://github.com/pranjalmaheshka/eco395m-sp23-DataMining/tree/main/Final-Project/stata) provided by the CDC and then the files were exported as .csv’s in order to process them in R. 
* In order to reproduce the results, save the csv files were saved locally due to their size and then run [data_cleaning.r](https://github.com/pranjalmaheshka/eco395m-sp23-DataMining/blob/main/Final-Project/data_cleaning.R). This file subsets the data down to relevant socio-demographic features, medical diagnoses, patient’s medical history, and type of care provided. 
+ Information about prescribed medication is cross-referenced with CDC data to determine which patients were given opioids including type and potency which were saved as `opioid` (=1 if prescribed an opioid), `potency` (measured relative to morphine = 1), `opioids` (total number of opioids prescribed). Two opioid classification information files needed to run the data cleaning script can be found [here]( https://github.com/pranjalmaheshka/eco395m-sp23-DataMining/tree/main/Final-Project/data). 
* The final cleaned file is available [here]( https://github.com/pranjalmaheshka/eco395m-sp23-DataMining/tree/main/Final-Project/data). 
The final data set includes 145,630 observations across 94 variables including 3 opioid variables generated. 

### Model Selection
1.	Random Forests: Chosen because it adds more randomness to our sample data the way we would expect to see in the real world. We consider a total of 83 features in the final model design so using random forests helps subset the number of features. 
2.	Logistic Regression: Chosen because a binary outcome is being considered here which is whether an opioid was prescribed or not.  **REVIEW**

**Model Training:** The data show that opioid prescriptions are relatively stable in 2018-2019 relative to the previous years. There might be a lag in broader adoption of the 2016 CDC guidelines and the prescriptions drop from 2013-2018. Therefore, 2018-2019 data were taken as “ideal” prescription rates and the model was trained on this data using an 80/20 train test split. The model was then run on data from 2013-2017 to determine overprescription of opioids in the past.   

**Model Evaluation:** Both models were evaluated using a confusion matrix focusing on their specificity, sensitivity, and precision. **(slide deck 04 – 31) MARCOOOO**

**Performance Optimization:** **MARCOOO**


```{r data_wrangling, include=FALSE}
factor(opioid_df$VDAYR)
factor(opioid_df$VMONTH)
factor(opioid_df$RACEUN)
factor(opioid_df$IMMEDR)

factor(opioid_df$PAYTYPER)
factor(opioid_df$RESIDNCE)
factor(opioid_df$REGION)
factor(opioid_df$ETHUN)
factor(opioid_df$MSA)

factor(opioid_df$CEBVD)
factor(opioid_df$EDHIV)
factor(opioid_df$NOCHRON)
factor(opioid_df$RFV1)
factor(opioid_df$DIAG1)

opioid_df$AGE<-ifelse(opioid_df$AGE=="Under one year",1, opioid_df$AGE)
opioid_df$AGE<-ifelse(opioid_df$AGE=="93 years and over",93, opioid_df$AGE)
opioid_df$SEX<-ifelse(opioid_df$SEX=="Female",1,0)

opioid_df$PAINSCALE<-ifelse(opioid_df$PAINSCALE=="Blank"|opioid_df$PAINSCALE=="Unknown",0,opioid_df$PAINSCALE)
opioid_df$PAINSCALE<-as.numeric(opioid_df$PAINSCALE)
opioid_df$AGE = as.numeric(opioid_df$AGE)

opioid_df = opioid_df %>%
  mutate(DIAG1 = ifelse(is.na(DIAG1), "BLANK", DIAG1))
opioid_df = opioid_df%>%
  mutate(DIAG2 = ifelse(is.na(DIAG2), "BLANK", DIAG2))
opioid_df = opioid_df%>%
  mutate(DIAG3 = ifelse(is.na(DIAG3), "BLANK", DIAG3))

factor(opioid_df$DIAG2)
factor(opioid_df$DIAG3)

opioid_df = opioid_df %>%
  mutate(pre2016 = ifelse(YEAR < 2016 | (YEAR==2016 & 
                                           (VMONTH == 'January' | VMONTH == 'February')), 1, 0),
         PAYMCARE = ifelse(PAYMCARE == "Yes", 1, 0),
         PAYPRIV = ifelse(PAYPRIV == "Yes", 1, 0))

```

## Results 


```{r random_forests1, include=TRUE, warning=FALSE, echo=FALSE, cache=TRUE}
opioid_model = subset(opioid_df, YEAR== 2018 | YEAR==2019)
opioid_model = na.omit(opioid_model)
opioid_split =  initial_split(opioid_model, prop=0.8)

traindata = training(opioid_split)
testdata  = testing(opioid_split)

load.forest = randomForest(opioid ~ . -RFV1
                           -RFV2 - RFV3 -opioids
                           -PAYPRIV - PAYMCARE - PAYMCAID - PAYWKCMP
                           -PAYSELF - PAYNOCHG - PAYOTH - PAYDK 
                           -PATCODE - BDATEFL - SEXFL - AGER -YEAR 
                           -RACER - RACERETH - ETHIM - AGEDAYS 
                           -potency,
                           data=traindata, importance = TRUE, ntree=100)
```

```{r, echo=FALSE, warning=FALSE}
#plot(load.forest)

modelr::rmse(load.forest, testdata) 

# variable importance measures
vi = varImpPlot(load.forest, type=1)

# partial dependence plots
partialPlot(load.forest, testdata, 'PAINSCALE', las=1)
partialPlot(load.forest, testdata, 'AGE', las=1)


testdata$predict_opioidn = predict(load.forest, testdata)
x <- testdata$opioid
y <- testdata$predict_opioidn
rocdata <- data.frame(x,y)
roc.plot(rocdata$x, rocdata$y, show.thres=FALSE)

```

```{r, echo=FALSE}
testdata = testdata %>%
  mutate(predict_opioid = ifelse(predict_opioidn >= 0.4, 1, 0), 
         false_pos = ifelse(predict_opioid > opioid, 1, 0))

table(real_opioid=testdata$opioid, predict_opioid=testdata$predict_opioid)

#Dumb model is 87.1% accurate

```

```{r, echo=FALSE}
post16_df= opioid_df %>%
   filter(YEAR == 2017 | YEAR == 2018 | YEAR == 2019)

post16_df$predict_opioid = predict(load.forest, post16_df)

post16_df = post16_df %>%
  mutate(predict_opioid = ifelse(predict_opioid >= 0.20, 1, 0), 
         false_neg = ifelse(predict_opioid < opioid, 1, 0))

table(real_opioid=post16_df$opioid, predict_opioid=post16_df$predict_opioid)

post16_df %>%
  filter(false_pos == 1) %>%
  summarize(perc_female = mean(SEX)*100, 
            mean_age = mean(AGE),
            perc_mcare = mean(PAYMCARE)*100,
            perc_privins = mean(PAYPRIV)*100,
            pain = mean(PAINSCALE))

```

```{r random_forests_treshold, include=TRUE}
testdata$predict_opioid = predict(load.forest, testdata)

## F1 test to choose treshhold
x <- factor(testdata$opioid)
y <- factor(testdata$predict_opioid)

confusionMatrix(y, x, mode = "everything", positive="1")

testdata = testdata %>%
  mutate(predict_opioid = ifelse(predict_opioid >= 0.17, 1, 0), 
         false_neg = ifelse(predict_opioid < opioid, 1, 0))

table(real_opioid=testdata$opioid, predict_opioid=testdata$predict_opioid)

## ROC Curve

#w <- (testdata$opioid)
#z <- (testdata$predict_opioid)
#rocdata <- data.frame(w,z)
#roc.plot(rocdata$w, rocdata$z, show.thres=FALSE)


```




## Conclusion
The opioid crisis has seen some respite by the suits against both manufacturers like Johnson & Johnson ([$26 billion settlement in 2021]( https://www.naag.org/issues/opioids/)) and retailers like Walmart ([$3.1 billion settlement in 2022]( https://www.nytimes.com/2022/11/15/health/walmart-opioids-settlement.html)) and some relief with changing prescription guidelines, the development of non-opioid pain relievers, and general awareness amongst the general public and medical practitioners. 

•	Our model build process and drawbacks

•	Main takeaways

#### Future Considerations

## Appendix
Optional



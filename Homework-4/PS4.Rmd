---
title: "HW4"
author: "Marco Navarro, Asha Christensen, Pranjal Maheshka"
date: "2023-04-17"
output: md_document
always_allow_html: true
---

# Data Mining PS4

## Pranjal Maheshka, Asha Christensen, Marco Navarro
### 2023-04-17

```{r q1_setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings=FALSE, message=FALSE)
library(ClusterR)
library(tidyverse)
library(cluster)
library(data.table)
library(gridExtra)
```

```{r q1_intro, include=FALSE}
wine <-  read.csv("wine.csv")
X2 <- na.omit(wine)
X2 = scale(X2[,-c(12,13)], center=TRUE, scale=TRUE)
mu = attr(X2,"scaled:center")
sigma = attr(X2,"scaled:scale")
```

## Question 1: Wine Analysis - Red or White? Good or Bad?

Using a data set containing 6500 types of wine and 11 chemical properties associated with each of them, we are trying to predict the color of the wine the quality of the wine using PCA and KMeans algorithms. Let us first consider some simple differences in the types of wine.

-   Red wine:
-   White wine:

Second, let us consider the quality of wine. Traditionally, wine is rated on a 100-point scale with the scores bucketed as shown below. However, this data set the wine is rated between 3-9 which might imply a scale of 1-10 but this information is unclear.

-   95-100: Classic
-   90-94: Outstanding
-   85-89: Very good
-   80-84: Solid
-   75-79: Mediocre
-   50-74: Not recommended


### Clustering
The first approach uses KMeans with 2 clusters to determine the types of wine and with 7 clusters to determine the quality of wine. A quick look at the 11 chemical properties for both clusters tells us a few key differences between red and white wine. Many of the properties are indistinguishable like alcohol content, pH levels, and density. 

- Red wine: 
    + Lower: Residual sugar content,  free and total sulfur dioxide, citric acid 
    + Higher: Chlorides
-   White wine:
    + Lower: Chlorides
    + Higher: Residual sugar content,  free and total sulfur dioxide, citric acid
    
Plotting pH vs Alcohol shows us that these attributes are not the right ones to be able to distinguish the differences in the wine. There is almost complete overlap and these cannot be used to predict the type of wine. 

```{r q1_kmeans1, echo=FALSE,warning=FALSE}
clust1 = kmeans(X2, 2, nstart=25)
check1 = clust1$center[1,]*sigma + mu
check2 = clust1$center[2,]*sigma + mu

qplot(alcohol, pH, data=wine, color=factor(clust1$cluster)) + scale_color_manual(values=c("orange", "brown"))
```

Plotting Total Sulfur Dioxide vs Citric Acid shows us a more distinct plot. Here, both wines have a range of citric acids but for citric acid > 0.8, it is usually white wine. The bottom left of the plot with lower citric acid and lower total sulfur dioxide shows the red wine. White wine tends to have higher total sulfur dioxide. 

```{r q1_kmeans2, echo=FALSE, warning=FALSE}
qplot(citric.acid, total.sulfur.dioxide, data=wine, color=factor(clust1$cluster)) + scale_color_manual(values=c("orange", "brown"))
```

Now, using KMeans for predicting wine quality by using 7 clusters. The plots below show that there are minimal to no discernible patterns in the clusters and they are not related to the actual quality of wine. Clustering is not a good method to determine the quality of wine especially when considering only the chemical properties of wine.

```{r q1_kmeans3, echo=FALSE,warning=FALSE}
KMeans_Quality = kmeans(X2, 7, nstart=25)

qplot(alcohol, pH, data=wine, color=factor(KMeans_Quality$cluster))
qplot(citric.acid, total.sulfur.dioxide, data=wine, color=factor(KMeans_Quality$cluster))

```


### PCA

First, we are trying to predict the color of the wine using PCA with Rank = 2 which was chosen as the simplest way to condense the 11 chemical properties in the data (higher ranks did not make any visible changes in prediction).

The plot below shows that this method is quite good at being able to distinguish between red and white wines. The simplified 2 component analysis makes it easy to visually distinguish between red and white wines.

```{r q1_pca1, echo=FALSE, warning=FALSE}
##  PCA - Color prediction
pca1 = prcomp(X2, scale=TRUE, rank=2)
loadings_type = pca1$rotation
scores_type = pca1$x

qplot(scores_type[,1], scores_type[,2], color=wine$color, xlab='Component 1', ylab='Component 2') + scale_color_manual(values=c("red", "darkslategray3"), name="Wine Color")
```

The plot below shows the quality of wine when considering PCA with Rank = 7. There is no discernible pattern and PCA is a not a good approach to predict the quality of wine based on the recorded chemical properties. This result remains true even if different components are used for plotting. 

```{r q1_pca2, echo=FALSE, warning=FALSE}
## PCA - Quality prediction 
pca2 = prcomp(X2, scale=TRUE, rank=7)
loadings_quality = pca2$rotation
scores_quality = pca2$x

qplot(scores_quality[,1], scores_quality[,2], color=wine$quality, xlab='Component 1', ylab='Component 2') + scale_color_continuous(name="Wine Quality")

```

### Conclusion

There are two considerations here - predicting the type of wine (red or white) and the predicting the quality of wine (3-9). PCA is a robust method at predicting the type of wine and the components make it very easy to determine red vs white wine. This method is far superior to clustering using KMeans where we have to look all differences in the chemical properties and try plotting different combinations of these variables in order to find a combination that makes it easier to determine the type of wine. Even when we consider total sulfur dioxide versus citric acid, there is still significant overlap. This is because the color of wine is determined by the way that the wine is fermented and whether the skin of the grape is part of this fermentation or not. The overall chemical composition of wine is similar in many ways and all types of wine are similar in the aspects of density, pH, and alcohol content. PCA allows the distilling of the 11 parameters into a relevant 2 component analysis. 

The quality of wine is a subjective measure of taste and feel (and sometimes also color) but the chemical composition of the wine itself is not a good indicator for this. Both clustering and PCA performed poorly in determining the quality of wine and there is no good visual way (using graphs) to differentiate quality based on chemical properties. 

In short, PCA is the better unsupervised learning technique for classifying types of wine. Both clustering and PCA are not good at determining the quality of wine given the chemical parameters being considered here. 



## Question 2: Market segmentation

```{r setup1, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ClusterR)  # for kmeans++
library(foreach)
library(mosaic)
library(cowplot)

library(igraph)
library(arules)  # has a big ecosystem of packages built around it
library(arulesViz)
```

In order to carry out this market segmentation exercise for NutrientH20, we used tweets from followers of the Twitter account of the brand. Each tweet was categorized based on its content using a pre-specified scheme of 36 different categories, each representing a broad area of interest (e.g. politics, sports, family, etc.).

First of all, we eliminate some categories of the data base that do not add interesting information, such as "spam", "adult" and "chatter". Then, we calculate the ratio between the number of posts by a given user that fell into the given category and the total number annotations for each user before centering and scaling the data.

```{r , include=FALSE}
################################### Read dataset
social = read.csv("social_marketing.csv")

social2 = social[,-c(1,2,36,37)]

Z = social2/rowSums(social2)

#social2 = social2 %>%
#  mutate(total = rowSums(social2))

ZZ = scale(Z, center=TRUE, scale=TRUE)

```

For the market segmentation we try to divide followers (probably costumers) into different groups according to similarities in their tweets categories. Once we have identified these groups, the company will have more knowledge about their followers (costumers) and design marketing strategies accordingly.

In order to separate followers in distinct groups, we use a popular unsupervised learning method known as K-means++. As a result of this process, we divide followers in 5 different clusters, and accounts whose tweets are classified to the same or similar categories end up in the same group, cluster or market segment. The elbow plot suggests more clusters, but we pick 5 in order to facilitate the interpretation of our results.

```{r , include=FALSE}
k_grid = seq(2, 30, by=1)
SSE_grid = foreach(k = k_grid, .combine='c') %do% {
cluster_k = kmeans(ZZ, k, nstart=50)
cluster_k$tot.withinss
}

plot(SSE_grid)
```

```{r , include=FALSE}
clust1 = KMeans_rcpp(ZZ, 5)
```

Once we have identified these 5 clusters, we need to associate each cluster to a category or set of categories in order to provide more useful information. To accomplish that, we perform a principal component analysis in order to know which principal component is more related to each cluster and then analyze the loadings principal components.

In the following graphs, we can visualize relationships between each cluster and the principal components. These relationships and the loadings of the principal components can be used to identify relevant categories for each cluster.

```{r , include=FALSE}
pc_Z = prcomp(ZZ, rank=10)

summary(pc_Z)

scores = as.data.frame(pc_Z$x)

loadings= pc_Z$rotation %>%
  as.data.frame() %>%
  rownames_to_column('Category')

loadings %>%
  select(Category, PC1) %>%
  arrange(desc(PC1))

Social3 =  cbind(ZZ,scores)
```

```{r , echo=FALSE}
## cluster  2 3 and 4 pc2
A= ggplot(Social3) + 
  geom_point(aes(PC2, PC1, color=factor(clust1$cluster)))+
  theme(legend.position = "none")

## cluster 1 and 5 pc3
B =ggplot(Social3) + 
  geom_point(aes(PC3, PC1, color=factor(clust1$cluster)))+
  theme(legend.position = "none")

## cluster 3 pc4
C =ggplot(Social3) + 
  geom_point(aes(PC5, PC4, color=factor(clust1$cluster)))+
  guides(color = guide_legend(title = "Clusters"))

library("cowplot")
ggdraw() +
  draw_plot(A, x = 0, y = .5, width = .5, height = .5) +
  draw_plot(B, x = .5, y = .5, width = .5, height = .5) +
  draw_plot(C, x = 0, y = 0, width = .6, height = 0.5)
```

|Category |Loadings PC1| Category |Loadings PC2| Category |Loadings PC3|Category |Loadings PC4|Category |Loadings PC5|    
| ----- |:---:|  ----- |:---:|  ----- |:---:| ----- |:---:| ----- |:---:|
|sports_fandom | 0.404690305|politics|0.261102608|politics|0.357194859|fashion|0.2486018310|fashion|0.286886643|
|religion | 0.400669823	|travel|0.236649408|news|0.329012235|beauty|0.2366545788|cooking|0.27238276|
|parenting | 0.378260522|	college_uni	|	0.203784704|health_nutrition|0.278810631|cooking|0.2320392878|beauty|0.261100664|
|food | 0.285342668	|	tv_film|0.174483882|personal_fitness|0.265768747|politics|0.2253890276|news|0.252758649|
|school | 0.254633402|	current_events|	0.153131682	|outdoors|0.264414042|photo_sharing|0.1936194074|online_gaming|0.246631776|
|.|	.|.|.|.|.|.|.|.|.|
|outdoors | -0.144632427|cooking|	-0.20634821|school|-0.155270893|food|-0.1071448600|crafts|-0.150803291|
|fashion|-0.183544726|food			|-0.231026889|photo_sharing|-0.224246899|tv_film|	-0.151685085|eco|-0.219445309|
|personal_fitness | -0.225382342|	outdoors|	-0.272709650	|	cooking|-0.243277175|sports_playing|-0.2903722124|current_events|-0.262704701|
|health_nutrition |-0.243124199		|personal_fitness|-0.381209631|beauty|-0.321209763|online_gaming|-0.4735595614|photo_sharing|-0.308125501|
|cooking| -0.275127013|health_nutrition|-0.404175549|fashion|-0.339373781|college_uni|-0.4931870468|shopping|-0.394375778|



The first cluster is associated with positive values of the second and third principal component (PC2 and PC3). Taking that information into account and looking at the loadings, we can conclude that important categories of this cluster are politics, travel, college_uni, tv_film, current_events, news, health_nutrition and personal_fitness. Consequently, this market segment includes young adults who are probably in college and are  political enthusiasts. They also have a healthy lifestyle and are also well aware of the current social and political problems.

The second cluster is associated with positive values of PC2 and negative values of PC5. In this cluster, the important categories are politics, travel, college_uni, tv_film, current_events, news, shopping, photo_sharing and eco. It is similar to the firs group (college students and interested in politics and current events), but they are probably also interested in spending their free time shopping, creating social media content instead of doing physical activities. 

The third cluster is associated with positive values of PC2 and PC5. For this cluster, important categories are politics, travel, college_uni,tv_film, current_events, news, fashion, cooking and beauty. Again, this group includes young adults in college, and they care about their appearance and political events. 

The fourth cluster is associated with negative values of PC2. The most important categories with negative loadings in this component are health_nutrition, personal_fitness, outdoors and cooking. So, we can conclude that one of the market segments is composed by people with an active and healthy lifestyle who spend a considerable part of their free time doing physical activities, and also cares about the type of food they consume.

The last cluster is associated to negative values of PC3. Categories with the lowest negative loadings in PC3 are fashion, beauty, cooking, photo_sharing, school and religion. As a result, we can conclude that one of the market segments includes adults with kids who spend a considerable amount of time sharing content on social media. In addition, they care about religion and are concerned about their own appearance.

The information about the clusters can provide a better understanding of the groups of costumers or potential ones, so the company is in a better position to successfully design marketing campaigns for these different groups.

## Question 3: Association rules for grocery purchases


```{r, echo=FALSE, warning=FALSE}

groceries = read.table('groceries.txt', header=FALSE, sep = "\n", fill=TRUE)

groceries = separate(groceries, V1, c("1", "2", "3", "4", "5","6","7","8","9","10", "11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32"), sep = ",")


#shopper 1 to start us off
grocerydf = as.data.frame(t(groceries[1, 1:32]))
grocerydf = na.omit(grocerydf)
grocerydf$shopper = 1
grocerydf = grocerydf %>%
  rename("good" = "1")

for (i in 2:length(groceries$"1")) {
tempdf = as.data.frame(t(groceries[i, 1:32]))
tempdf = na.omit(tempdf)
tempdf$shopper = i
tempdf = tempdf %>%
  rename("good" = toString(i))
grocerydf = rbind(grocerydf, tempdf)
}

grocerydf$shopper = factor(grocerydf$shopper)
grocerydf = split(x=grocerydf$good, f=grocerydf$shopper)


```

```{r, echo=FALSE, include=FALSE}
grocerydf = lapply(grocerydf, unique)

grocerytrans = as(grocerydf, "transactions")
summary(grocerytrans)

```

```{r, echo=FALSE, include=FALSE}

groceryrules2 = apriori(grocerytrans, parameter=list(support=.01, confidence=.5, maxlen=4))
groceryrules1 = apriori(grocerytrans, parameter=list(support=.005, confidence=.05, maxlen=4))
```

First, let's create a very broad associations model by using thresholds that are very low (support = 0.005, confidence = 0.05). We allow up to 4 goods to be correlated: 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
plot(groceryrules1)

# can swap the axes and color scales
plot(groceryrules1, measure = c("support", "lift"), shading = "confidence")

# "two key" plot: coloring is by size (order) of item set
plot(groceryrules1, method='two-key plot')

# graph-based visualization
# export
# associations are represented as edges
# For rules, each item in the LHS is connected
# with a directed edge to the item in the RHS. 

```
```{r, echo = FALSE}

subrules = head(groceryrules1, n=10, by="lift")
plot(subrules, method = "graph", engine = "htmlwidget")
plot(subrules, method="paracoord")

```

Here we can see strong 2-way rule associations: ham and white bread, tropical fruit and whole milk, butter and whipped cream, etc. However, it may be the case that whole milk and tropical fruit are just 2 of the most common items, meaning that we are not really capturing an association, but rather just a result of this dual commonality, as seen below.

```{r, echo=FALSE}
itemFrequencyPlot(grocerytrans, topN=10,  cex.names=1)

```

If we are using this data analysis for a practical purpose, such as marketing, then we will want these rules to be relevant to a larger number of individuals and more true for those individuals. This means that we should set our confidence and support higher: I don't care about rules which impact less than 1% of consumers, and I don't care about rules which are true less than 50% of the time. Therefore, we can rerun the apriori function with support = 0.01 and confidence = 0.5:

```{r, echo=FALSE, warning=FALSE}
plot(groceryrules2)

# can swap the axes and color scales
plot(groceryrules2, measure = c("support", "lift"), shading = "confidence")

# "two key" plot: coloring is by size (order) of item set
plot(groceryrules2, method='two-key plot')

# graph-based visualization
# export
# associations are represented as edges
# For rules, each item in the LHS is connected
# with a directed edge to the item in the RHS. 

```

We do have much less rules, however, the rules that remain are applicable to a larger portion of shoppers and are more strongly true, and are therefore more relevant to application
```{r, include=FALSE}
subrules = head(groceryrules2, n=10, by="lift")
```

```{r, echo=FALSE}
plot(subrules, method = "graph", engine = "htmlwidget")
plot(subrules, method="paracoord")

```
